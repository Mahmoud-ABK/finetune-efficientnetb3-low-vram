{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5bd7ab-00b8-4bda-9a6e-2096f08416da",
   "metadata": {},
   "source": [
    "# Guide Dog Classifier: Fine-Tuning Without Optimization  \n",
    "\n",
    "**Objectives**  \n",
    "- Load the pre-trained model with frozen-head weights  \n",
    "- Unfreeze Convolutional layers and fine-tune on our dataset with low training weight\n",
    "- Encounter and analyze OOM errors on 4 GB GPU  \n",
    "- Profile VRAM usage with NVIDIA-SMI  \n",
    "- reset cache/kernel  \n",
    "\n",
    "Let’s start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ad0cc-00d2-4dc7-9342-81e703d74481",
   "metadata": {},
   "source": [
    "# Importing necessary libraries (again) , set up DataLoader with transfroms etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80769d7-2aee-4b02-9dd8-fd77dc7d5211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:34.441253Z",
     "iopub.status.busy": "2025-06-03T16:16:34.441046Z",
     "iopub.status.idle": "2025-06-03T16:16:39.319218Z",
     "shell.execute_reply": "2025-06-03T16:16:39.318850Z",
     "shell.execute_reply.started": "2025-06-03T16:16:34.441240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core Python and system utilities\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Numerical and data handling£\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0847875-6669-4ff6-9122-9d5463da0381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:39.320099Z",
     "iopub.status.busy": "2025-06-03T16:16:39.319836Z",
     "iopub.status.idle": "2025-06-03T16:16:39.479270Z",
     "shell.execute_reply": "2025-06-03T16:16:39.478917Z",
     "shell.execute_reply.started": "2025-06-03T16:16:39.320083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Initial VRAM Allocated: 0.00 MB\n",
      "Initial VRAM Reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# CUDA check and setup\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.cuda.empty_cache()  # Clear any residual memory\n",
    "    torch.backends.cudnn.benchmark = False  # Disable for deterministic results\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Initial VRAM Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
    "    print(f\"Initial VRAM Reserved: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU. Performance may be slow.\")\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_root = \"./dataset\"\n",
    "train_path = os.path.join(dataset_root, \"train\")\n",
    "val_path = os.path.join(dataset_root, \"val\")\n",
    "test_path = os.path.join(dataset_root, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d689da-ed0a-4172-8a0c-a4c2df393f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:39.479820Z",
     "iopub.status.busy": "2025-06-03T16:16:39.479678Z",
     "iopub.status.idle": "2025-06-03T16:16:39.483458Z",
     "shell.execute_reply": "2025-06-03T16:16:39.482984Z",
     "shell.execute_reply.started": "2025-06-03T16:16:39.479808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the transforms from the model weights \n",
    "transform = models.EfficientNet_B3_Weights.IMAGENET1K_V1.transforms()\n",
    "# Define custom training transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomResizedCrop(300, scale=(0.8, 1.0)),\n",
    "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET), #Applies learned augmentation policies for robustness.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379a50de-0144-4ee0-9b5e-d97fb02e8bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:39.484443Z",
     "iopub.status.busy": "2025-06-03T16:16:39.484286Z",
     "iopub.status.idle": "2025-06-03T16:16:39.502335Z",
     "shell.execute_reply": "2025-06-03T16:16:39.501887Z",
     "shell.execute_reply.started": "2025-06-03T16:16:39.484428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2016\n",
      "Validation samples: 224\n",
      "Test samples: 572\n",
      "Classes: ['guide_dogs', 'non_guide_dogs']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets using ImageFolder \n",
    "train_dataset = datasets.ImageFolder(train_path)\n",
    "val_dataset = datasets.ImageFolder(val_path)\n",
    "test_dataset = datasets.ImageFolder(test_path)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "num_workers = 1  # number of subprocesses\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "train_dataset.transform = train_transform\n",
    "val_dataset.transform = transform\n",
    "test_dataset.transform = transform\n",
    "\n",
    "# Verify dataset sizes and classes\n",
    "try:\n",
    "    assert len(train_dataset.classes) == 2, \"Expected 2 classes: guide_dogs, non_guide_dogs\"\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Classes: {train_dataset.classes}\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Dataset Error: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efc718-7961-44bc-886d-b0438224de58",
   "metadata": {},
   "source": [
    "## Configuring EfficientNet-B3 for Fine-Tuning  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f7453-2e40-4118-8b88-2eef954404d2",
   "metadata": {},
   "source": [
    "#### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51569a8-7e46-4752-8e1f-e662177cab9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:39.776537Z",
     "iopub.status.busy": "2025-06-03T16:16:39.776277Z",
     "iopub.status.idle": "2025-06-03T16:16:40.203385Z",
     "shell.execute_reply": "2025-06-03T16:16:40.203004Z",
     "shell.execute_reply.started": "2025-06-03T16:16:39.776512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.efficientnet_b3(weights=None)\n",
    "# replace classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1536, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 1)\n",
    ")\n",
    "# load frozen-head weights\n",
    "model.load_state_dict(torch.load(\"./model/frozen_model.pth\",weights_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e26e4b-f912-421a-96fa-aa069c3de696",
   "metadata": {},
   "source": [
    "### actually how does model loading works? I'll explain \n",
    "- first, we define the skeleton of the model (aka the structure) \n",
    "- the weights are imported from the dict as they are saved previously in last notebook using the `torch.save(model.state_dict(), \"./model/frozen_model.pth\")`\n",
    "- for security reasons we'll load only the weights from that state dict from last time </br>\n",
    "**Note** : as you can see we didn't load the pre-trained weights from the original model as we did in first time `model = models.efficientnet_b0(weights='IMAGENET1K_V1')` this time we ignored the parameter `weights='IMAGENET1K_V1'` and replaced it with `weights=None` to ensure no reloading of pretrained model.\n",
    "  <br> **But why?** <br>\n",
    " the `torch.save` that we called saved the model weights efficiently `model.load_state_dict(torch.load(\"./model/frozen_model.pth\",weights_only=True))` will load all the weights (including the one that were freezed so no need to load them twice.\n",
    "by default , the `weight` parameter is `None` but I set it anyways. \n",
    "\n",
    "actually the process of saving and loading the model falls under a broader term called *serialization/deserialization* .A topic you can explore yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb8728-1e6a-43e8-8859-e8de07ed5dd7",
   "metadata": {},
   "source": [
    "#### Let's review which layers we'll unfreeze to fine tune this model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc18688-153b-4829-808b-a10a5c6bfc2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:44.327890Z",
     "iopub.status.busy": "2025-06-03T16:16:44.327689Z",
     "iopub.status.idle": "2025-06-03T16:16:44.330270Z",
     "shell.execute_reply": "2025-06-03T16:16:44.329773Z",
     "shell.execute_reply.started": "2025-06-03T16:16:44.327877Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c57cc6-42bf-44ee-b2cc-f580b1b03dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:44.948855Z",
     "iopub.status.busy": "2025-06-03T16:16:44.948450Z",
     "iopub.status.idle": "2025-06-03T16:16:44.951876Z",
     "shell.execute_reply": "2025-06-03T16:16:44.951412Z",
     "shell.execute_reply.started": "2025-06-03T16:16:44.948840Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b1f6433-8825-44dc-9cc4-7c76ceabe41f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:45.718246Z",
     "iopub.status.busy": "2025-06-03T16:16:45.717780Z",
     "iopub.status.idle": "2025-06-03T16:16:45.724358Z",
     "shell.execute_reply": "2025-06-03T16:16:45.723832Z",
     "shell.execute_reply.started": "2025-06-03T16:16:45.718232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, True) (1, True) (2, True) (3, True) (4, True) (5, True) (6, True) (7, True) (8, True) (9, True) (10, True) (11, True) (12, True) (13, True) (14, True) (15, True) (16, True) (17, True) (18, True) (19, True) (20, True) (21, True) (22, True) (23, True) (24, True) (25, True) (26, True) (27, True) (28, True) (29, True) (30, True) (31, True) (32, True) (33, True) (34, True) (35, True) (36, True) (37, True) (38, True) (39, True) (40, True) (41, True) (42, True) (43, True) (44, True) (45, True) (46, True) (47, True) (48, True) (49, True) (50, True) (51, True) (52, True) (53, True) (54, True) (55, True) (56, True) (57, True) (58, True) (59, True) (60, True) (61, True) (62, True) (63, True) (64, True) (65, True) (66, True) (67, True) (68, True) (69, True) (70, True) (71, True) (72, True) (73, True) (74, True) (75, True) (76, True) (77, True) (78, True) (79, True) (80, True) (81, True) (82, True) (83, True) (84, True) (85, True) (86, True) (87, True) (88, True) (89, True) (90, True) (91, True) (92, True) (93, True) (94, True) (95, True) (96, True) (97, True) (98, True) (99, True) (100, True) (101, True) (102, True) (103, True) (104, True) (105, True) (106, True) (107, True) (108, True) (109, True) (110, True) (111, True) (112, True) (113, True) (114, True) (115, True) (116, True) (117, True) (118, True) (119, True) (120, True) (121, True) (122, True) (123, True) (124, True) (125, True) (126, True) (127, True) (128, True) (129, True) (130, True) (131, True) (132, True) (133, True) (134, True) (135, True) (136, True) (137, True) (138, True) (139, True) (140, True) (141, True) (142, True) (143, True) (144, True) (145, True) (146, True) (147, True) (148, True) (149, True) (150, True) (151, True) (152, True) (153, True) (154, True) (155, True) (156, True) (157, True) (158, True) (159, True) (160, True) (161, True) (162, True) (163, True) (164, True) (165, True) (166, True) (167, True) (168, True) (169, True) (170, True) (171, True) (172, True) (173, True) (174, True) (175, True) (176, True) (177, True) (178, True) (179, True) (180, True) (181, True) (182, True) (183, True) (184, True) (185, True) (186, True) (187, True) (188, True) (189, True) (190, True) (191, True) (192, True) (193, True) (194, True) (195, True) (196, True) (197, True) (198, True) (199, True) (200, True) (201, True) (202, True) (203, True) (204, True) (205, True) (206, True) (207, True) (208, True) (209, True) (210, True) (211, True) (212, True) (213, True) (214, True) (215, True) (216, True) (217, True) (218, True) (219, True) (220, True) (221, True) (222, True) (223, True) (224, True) (225, True) (226, True) (227, True) (228, True) (229, True) (230, True) (231, True) (232, True) (233, True) (234, True) (235, True) (236, True) (237, True) (238, True) (239, True) (240, True) (241, True) (242, True) (243, True) (244, True) (245, True) (246, True) (247, True) (248, True) (249, True) (250, True) (251, True) (252, True) (253, True) (254, True) (255, True) (256, True) (257, True) (258, True) (259, True) (260, True) (261, True) (262, True) (263, True) (264, True) (265, True) (266, True) (267, True) (268, True) (269, True) (270, True) (271, True) (272, True) (273, True) (274, True) (275, True) (276, True) (277, True) (278, True) (279, True) (280, True) (281, True) (282, True) (283, True) (284, True) (285, True) (286, True) (287, True) (288, True) (289, True) (290, True) (291, True) (292, True) (293, True) (294, True) (295, True) (296, True) (297, True) (298, True) (299, True) (300, True) (301, True) (302, True) (303, True) (304, True) (305, True) (306, True) (307, True) (308, True) (309, True) (310, True) (311, True) (312, True) (313, True) (314, True) (315, True) (316, True) (317, True) (318, True) (319, True) (320, True) (321, True) (322, True) (323, True) (324, True) (325, True) (326, True) (327, True) (328, True) (329, True) (330, True) (331, True) (332, True) (333, True) (334, True) (335, True) (336, True) (337, True) (338, True) (339, True) (340, True) (341, True) (342, True) (343, True) "
     ]
    }
   ],
   "source": [
    "for idx, param in enumerate(model.parameters()):\n",
    "    print(f\"({idx}, {param.requires_grad})\", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1317a336-e130-4874-ad10-68ffa780ea77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:50.377889Z",
     "iopub.status.busy": "2025-06-03T16:16:50.377680Z",
     "iopub.status.idle": "2025-06-03T16:16:50.405678Z",
     "shell.execute_reply": "2025-06-03T16:16:50.405179Z",
     "shell.execute_reply.started": "2025-06-03T16:16:50.377875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 11090473\n"
     ]
    }
   ],
   "source": [
    "# Send to Gpu\n",
    "model = model.to(device)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267c25a7-3bb5-49ef-aea1-049d795be8d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:51.247512Z",
     "iopub.status.busy": "2025-06-03T16:16:51.247149Z",
     "iopub.status.idle": "2025-06-03T16:16:51.440411Z",
     "shell.execute_reply": "2025-06-03T16:16:51.439870Z",
     "shell.execute_reply.started": "2025-06-03T16:16:51.247498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  3 17:16:51 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P0             11W /   65W |     157MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2448      G   /usr/bin/gnome-shell                      1MiB |\n",
      "|    0   N/A  N/A            7263      C   ...ptimizedfinetuning/bin/python        138MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85add38-1a00-4b3d-aed9-caf6ffa96a7f",
   "metadata": {},
   "source": [
    "# Fine-Tuning and OOM Demonstration\n",
    "\n",
    "We fine-tune the model with:\n",
    "- **Loss**: Binary Cross-Entropy (`BCELoss`).\n",
    "- **Optimizer**: Adam with learning rate 0.0001 (lower for fine-tuning).\n",
    "- **Batch Size**: 32 to increase VRAM demand.\n",
    "- **Epochs**: 5 (if successful).\n",
    "\n",
    "With unfrozen layers, VRAM usage may be intense, causing OOM on our 4GB GPU.\n",
    "\n",
    "**Note**: If training succeeds (unlikely), accuracy may reach ~85–90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c8e31-32d2-4883-988f-0ce5cb94819a",
   "metadata": {},
   "source": [
    "## memory profiling function  \n",
    "a small helper to profile memory usage during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ccccf2-22d5-40ff-b3da-5ccc7c33a051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:56.148733Z",
     "iopub.status.busy": "2025-06-03T16:16:56.148547Z",
     "iopub.status.idle": "2025-06-03T16:16:56.152826Z",
     "shell.execute_reply": "2025-06-03T16:16:56.152485Z",
     "shell.execute_reply.started": "2025-06-03T16:16:56.148719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Allocated Memory    : 42.78 MB   Reserved Memory      : 58.00 MB   Peak Allocated Memory: 42.78 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def memorytracking() :\n",
    "    used_mem = torch.cuda.memory_allocated()\n",
    "    reserved_mem = torch.cuda.memory_reserved()\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"  Allocated Memory    : {used_mem / (1024 ** 2):.2f} MB\" , end=' ') \n",
    "    print(f\"  Reserved Memory      : {reserved_mem / (1024 ** 2):.2f} MB\" , end=' ')\n",
    "    print(f\"  Peak Allocated Memory: {peak_mem / (1024 ** 2):.2f} MB\")\n",
    "memorytracking()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bba373-8382-4fcf-839f-81454b71d6ba",
   "metadata": {},
   "source": [
    "# The Training Loop \n",
    "here we set the learning late low so can it'll adjust to the data without drastically changing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2eb021e-31cb-4dfd-b032-8e20cdbe6a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:57.306567Z",
     "iopub.status.busy": "2025-06-03T16:16:57.306079Z",
     "iopub.status.idle": "2025-06-03T16:16:57.314545Z",
     "shell.execute_reply": "2025-06-03T16:16:57.314147Z",
     "shell.execute_reply.started": "2025-06-03T16:16:57.306553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds, train_targets = [], []\n",
    "      \n",
    "        \n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.float().to(device).view(-1, 1)  # Float targets, shape (batch_size, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)  # Output: (batch_size, 1) logits\n",
    "            loss = criterion(output, target)  # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Apply sigmoid for predictions\n",
    "            probs = torch.sigmoid(output)\n",
    "            preds = (probs > 0.5).float()  # Threshold at 0.5\n",
    "            train_preds.extend(preds.cpu().numpy().flatten())\n",
    "            train_targets.extend(target.cpu().numpy().flatten())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.float().to(device).view(-1, 1)\n",
    "                output = model(data)  # (batch_size, 1) logits\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(output)\n",
    "                preds = (probs > 0.5).float()\n",
    "                val_preds.extend(preds.cpu().numpy().flatten())\n",
    "                val_targets.extend(target.cpu().numpy().flatten())\n",
    "\n",
    "        # Metrics\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        memorytracking()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac8461f-446c-41df-8963-62c77c4b3048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:16:58.408230Z",
     "iopub.status.busy": "2025-06-03T16:16:58.408045Z",
     "iopub.status.idle": "2025-06-03T16:17:00.751622Z",
     "shell.execute_reply": "2025-06-03T16:17:00.750873Z",
     "shell.execute_reply.started": "2025-06-03T16:16:58.408219Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacity of 3.69 GiB of which 154.94 MiB is free. Including non-PyTorch memory, this process has 3.52 GiB memory in use. Of the allocated memory 3.41 GiB is allocated by PyTorch, and 15.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Attempt to train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     16\u001b[39m data, target = data.to(device), target.float().to(device).view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Float targets, shape (batch_size, 1)\u001b[39;00m\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output: (batch_size, 1) logits\u001b[39;00m\n\u001b[32m     19\u001b[39m loss = criterion(output, target)  \u001b[38;5;66;03m# BCEWithLogitsLoss\u001b[39;00m\n\u001b[32m     20\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torchvision/models/efficientnet.py:343\u001b[39m, in \u001b[36mEfficientNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torchvision/models/efficientnet.py:333\u001b[39m, in \u001b[36mEfficientNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n\u001b[32m    336\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torchvision/models/efficientnet.py:164\u001b[39m, in \u001b[36mMBConv.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_res_connect:\n\u001b[32m    166\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.stochastic_depth(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/optimizedfinetuning/lib/python3.11/site-packages/torch/nn/functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacity of 3.69 GiB of which 154.94 MiB is free. Including non-PyTorch memory, this process has 3.52 GiB memory in use. Of the allocated memory 3.41 GiB is allocated by PyTorch, and 15.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Attempt to train\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920e986-c72a-4eb6-80ef-4d2d7e91d974",
   "metadata": {},
   "source": [
    "# OOM in a nutshell \n",
    "here is the output of my error <br>\n",
    "![OOM](./images/OOM.png)\n",
    "\n",
    "I guess it is explanatory enough , we basically exceeded the allowed range of 4GB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7da3d-c3c7-4641-83fb-ad5c299e5ad5",
   "metadata": {},
   "source": [
    "## Memory Profiling with NVIDIA-SMI\n",
    "\n",
    "We use NVIDIA-SMI to check VRAM usage during fine-tuning. Unfreezing the last convolutional block and using a batch size of 32 likely pushed usage to ~7GB, exceeding our 4GB RTX 3050 Ti’s limit, causing the OOM error.\n",
    "\n",
    "Run `nvidia-smi` to inspect GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "797ec307-9de5-439f-9b2f-14e5f8263107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T16:17:08.378648Z",
     "iopub.status.busy": "2025-06-03T16:17:08.378296Z",
     "iopub.status.idle": "2025-06-03T16:17:08.556421Z",
     "shell.execute_reply": "2025-06-03T16:17:08.555900Z",
     "shell.execute_reply.started": "2025-06-03T16:17:08.378633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  3 17:17:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P0             11W /   65W |    3627MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2448      G   /usr/bin/gnome-shell                      1MiB |\n",
      "|    0   N/A  N/A            7263      C   ...ptimizedfinetuning/bin/python       3608MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf768-9163-44a7-93ba-d4d3013bb39a",
   "metadata": {},
   "source": [
    "## Successful Run on a Powerful GPU\n",
    "\n",
    "On a more powerful GPU (e.g., 8GB RTX 3080 or cloud instance like AWS V100), fine-tuning with unfrozen layers and batch size 32 typically succeeds without OOM errors. <br>\n",
    "Since our 4GB GPU hit OOM, we’ll address this in Notebook 4 with optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb50ff-e4a0-41a5-a94e-243daef8e195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T23:37:53.932584Z",
     "iopub.status.busy": "2025-05-11T23:37:53.932314Z",
     "iopub.status.idle": "2025-05-11T23:37:53.936526Z",
     "shell.execute_reply": "2025-05-11T23:37:53.935919Z",
     "shell.execute_reply.started": "2025-05-11T23:37:53.932557Z"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "If fine-tuning completed any epochs before OOM, we evaluate the model on the test set (560 images). If OOM occurred early, evaluation will be deferred to Notebook 4 after optimization.\n",
    "\n",
    "Metrics:\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4d382-f684-4e97-ba00-ed864b270df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.float().to(device).view(-1, 1)\n",
    "            output = model(data)\n",
    "            preds.extend((output > 0.5).float().cpu().numpy().flatten())\n",
    "            targets.extend(target.cpu().numpy().flatten())\n",
    "    \n",
    "    acc = accuracy_score(targets, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average=\"binary\")\n",
    "    return acc, precision, recall, f1, preds, targets\n",
    "\n",
    "# Evaluate\n",
    "test_acc, test_precision, test_recall, test_f1, test_preds, test_targets = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f61714-51eb-403b-b831-da9e10a4b2dc",
   "metadata": {},
   "source": [
    "## Training Curves\n",
    "\n",
    "If fine-tuning ran for any epochs, we plot training and validation loss/accuracy to assess progress. If OOM stopped training early, this will be addressed in Notebook 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79178db-04c9-4e9b-a438-8e9f97dd5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_acc\"], label=\"Training Accuracy\")\n",
    "plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b797c-1443-4037-b367-688bc1133f51",
   "metadata": {},
   "source": [
    "## Reset Cache and Shut Down Kernel\n",
    "\n",
    "To ensure a clean state, we clear GPU memory and shut down the kernel. After running the cell below, the kernel will stop. Open `3_Fine_Tuning_Memory_Optimization.ipynb` in a new Jupyter session to apply memory optimization.\n",
    "\n",
    "**Note**: Restart Jupyter or open the next notebook manually after shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb55156-4628-4d3d-a1cc-ee5f231cd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared.\")\n",
    "\n",
    "# Shut down the kernel\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimized fine tuning",
   "language": "python",
   "name": "optimizedfinetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
